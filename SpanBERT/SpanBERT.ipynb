{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2957600c-b76c-42f8-bf0c-514ea7282c27",
   "metadata": {},
   "source": [
    "## SpanBERT model training, contents include\n",
    "#### 1. Training data preparation\n",
    "    x: Input features are text (dataset_dict[\"input_ids\"], dataset_dict[\"token_type_ids\"], dataset_dict[\"attention_mask\"]) and sentiment label\n",
    "    y: Selected text/answer\n",
    "#### 2. Model training\n",
    "#### 3. Inference\n",
    "**Note: refer to original post for detail https://www.kaggle.com/code/anasofiauzsoy/tweet-sentiment-extraction-with-tf2-spanbert/notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06ec7c5-9111-4853-b524-30fbc9ccc36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle/input/tweet-sentiment-extraction/test.csv\n",
      "kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n",
      "kaggle/input/tweet-sentiment-extraction/train.csv\n",
      "kaggle/input/spanbert-pt/spanbert-base-cased/config.json\n",
      "kaggle/input/spanbert-pt/spanbert-base-cased/pytorch_model.bin\n",
      "kaggle/input/spanbert-pt/spanbert-base-cased/vocab.txt\n",
      "kaggle/input/spanbert-pt/spanbert-large-cased/config.json\n",
      "kaggle/input/spanbert-pt/spanbert-large-cased/pytorch_model.bin\n",
      "kaggle/input/spanbert-pt/spanbert-large-cased/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        if 'checkpoint' not in  filename:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75e3dbf8-f186-4970-9d8c-05fe2f20b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 08:37:49.168812: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-17 08:37:49.168857: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/opt/cortex-installs/miniconda/envs/cortex-python3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig, BertTokenizerFast \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b60581af-9a23-43ea-a1bd-ed70805ab8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: (27481, 4); test size: (3534, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"kaggle/input/tweet-sentiment-extraction/train.csv\")\n",
    "test = pd.read_csv(\"kaggle/input/tweet-sentiment-extraction/test.csv\")\n",
    "print('training size: {}; test size: {}'.format(data.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d71413bc-128c-440c-98f2-0e90873c16af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                       my boss is bullying me...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.head(3))\n",
    "display(test.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36967974-3798-4f87-8d5e-725394cb815f",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d12e4246-847f-4cc3-b943-d7325c51153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16363, 4)\n",
      "(16363, 4)\n"
     ]
    }
   ],
   "source": [
    "data = data[data.sentiment != \"neutral\"]\n",
    "print(data.shape)\n",
    "data = data[pd.notnull(data.selected_text)]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aced4934-03f3-4852-ac06-86b01485f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 384\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"kaggle/input/spanbert-pt/spanbert-base-cased/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc110173-2c55-41f1-9b32-fb6342ecddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTweetExample:\n",
    "    def __init__(self, sentiment, text, start_char_idx, selected_text, all_answers):\n",
    "        self.sentiment = sentiment\n",
    "        self.text = text\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.selected_text = selected_text\n",
    "        self.all_answers = all_answers\n",
    "        self.skip = False\n",
    "\n",
    "    def preprocess(self):\n",
    "        text = self.text\n",
    "        sentiment = self.sentiment\n",
    "        selected_text = self.selected_text\n",
    "        start_char_idx = self.start_char_idx\n",
    "\n",
    "        # Clean text, sentiment, and answer\n",
    "        sentiment = \" \".join(str(sentiment).split())\n",
    "        text = \" \".join(str(text).split())\n",
    "        answer = \" \".join(str(selected_text).split())\n",
    "\n",
    "        # Find end character index of answer in text\n",
    "        end_char_idx = start_char_idx + len(answer)\n",
    "        if end_char_idx >= len(text):\n",
    "            self.skip = True\n",
    "            return\n",
    "\n",
    "        # Mark the character indexes in text that are in answer\n",
    "        is_char_in_ans = [0] * len(text)\n",
    "        for idx in range(start_char_idx, end_char_idx):\n",
    "            is_char_in_ans[idx] = 1\n",
    "\n",
    "        # Tokenize text\n",
    "        tokenized_text = tokenizer.encode_plus(text, return_offsets_mapping=True, max_length = max_len)\n",
    "\n",
    "        # Find tokens that were created from answer characters\n",
    "        ans_token_idx = []\n",
    "        for idx, (start, end) in enumerate(tokenized_text.offset_mapping):\n",
    "            if sum(is_char_in_ans[start:end]) > 0:\n",
    "                ans_token_idx.append(idx)\n",
    "\n",
    "        if len(ans_token_idx) == 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "\n",
    "        # Find start and end token index for tokens from answer\n",
    "        start_token_idx = ans_token_idx[0]\n",
    "        end_token_idx = ans_token_idx[-1]\n",
    "\n",
    "        # Tokenize sentiment\n",
    "        tokenized_sentiment = tokenizer.encode_plus(sentiment, return_offsets_mapping=True, max_length = max_len)\n",
    "\n",
    "        # Create inputs\n",
    "        input_ids = tokenized_text.input_ids + tokenized_sentiment.input_ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_text.input_ids) + [1] * len(\n",
    "            tokenized_sentiment.input_ids[1:]\n",
    "        )\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Pad and create attention masks.\n",
    "        # Skip if truncation is needed\n",
    "        padding_length = max_len - len(input_ids)\n",
    "        if padding_length > 0:  # pad\n",
    "            input_ids = input_ids + ([0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        elif padding_length < 0:  # skip\n",
    "            self.skip = True\n",
    "            return\n",
    "\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.start_token_idx = start_token_idx\n",
    "        self.end_token_idx = end_token_idx\n",
    "        self.context_token_to_char = tokenized_text.offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3f5f92d-f5a6-414c-aa69-765eb1f78aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tweet_examples(data):\n",
    "    tweet_examples = []\n",
    "    all_answers = data.selected_text.values\n",
    "    count = 0\n",
    "    for index, row in data.iterrows():\n",
    "        sentiment = row['sentiment']\n",
    "        text = row['text']\n",
    "        selected_text = row['selected_text']\n",
    "\n",
    "        try: ## turn the data into TrainTweetExample objects\n",
    "            start_char_idx = row.text.index(row.selected_text.split()[0])\n",
    "            tweet_eg = TrainTweetExample(\n",
    "                sentiment, text, start_char_idx, selected_text, all_answers\n",
    "            )\n",
    "            tweet_eg.preprocess()\n",
    "            tweet_examples.append(tweet_eg)\n",
    "        except: ## keep track of the number that can't be tokenized/processed\n",
    "            count += 1 \n",
    "    print(\"Couldn't process\",count,\"points\")\n",
    "    return tweet_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2436e0ae-ecb8-4220-b034-cef80a1e5e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTweetExample:\n",
    "    def __init__(self, sentiment, text):\n",
    "        self.sentiment = sentiment\n",
    "        self.text = text\n",
    "        self.skip = False\n",
    "\n",
    "    def preprocess(self):\n",
    "        text = self.text\n",
    "        sentiment = self.sentiment\n",
    "\n",
    "        # Clean text, answer and sentiment\n",
    "        sentiment = \" \".join(str(sentiment).split())\n",
    "        text = \" \".join(str(text).split())\n",
    "\n",
    "        # Tokenize text\n",
    "        tokenized_text = tokenizer.encode_plus(text, return_offsets_mapping=True, max_length = max_len)\n",
    "\n",
    "        # Tokenize sentiment\n",
    "        tokenized_sentiment = tokenizer.encode_plus(sentiment, return_offsets_mapping=True, max_length = max_len)\n",
    "\n",
    "        # Create inputs\n",
    "        input_ids = tokenized_text.input_ids + tokenized_sentiment.input_ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_text.input_ids) + [1] * len(\n",
    "            tokenized_sentiment.input_ids[1:]\n",
    "        )\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Pad and create attention masks.\n",
    "        # Skip if truncation is needed\n",
    "        padding_length = max_len - len(input_ids)\n",
    "        if padding_length > 0:  # pad\n",
    "            input_ids = input_ids + ([0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        elif padding_length < 0:  # skip\n",
    "            self.skip = True\n",
    "            return\n",
    "\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.text_token_to_char = tokenized_text.offset_mapping\n",
    "        self.start_token_idx = 0\n",
    "        self.end_token_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "556e8b3b-72bc-4ae0-9a4d-3c4ef57d9114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tweet_examples_test(data):\n",
    "    tweet_examples = []\n",
    "    all_answers = None\n",
    "    count = 0\n",
    "    for index, row in data.iterrows():\n",
    "        sentiment = row['sentiment']\n",
    "        text = row['text']\n",
    "        selected_text = None\n",
    "\n",
    "        try: ## turn the data into TestTweetExample objects\n",
    "            start_char_idx = 0\n",
    "            tweet_eg = TestTweetExample(\n",
    "                sentiment, text\n",
    "            )\n",
    "            tweet_eg.preprocess()\n",
    "            tweet_examples.append(tweet_eg)\n",
    "        except: ## keep track of the number that can't be tokenized/processed\n",
    "            count += 1\n",
    "    print(\"Couldn't process\",count,\"points\")\n",
    "    return tweet_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b5fe4e2-e06c-40d6-86cc-ef090f008e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs_targets(tweet_examples):\n",
    "    \"\"\"Create data inputs to SpanBERT model\n",
    "    \n",
    "    Return:\n",
    "            x: list - input features\n",
    "            y: list - response with length 2 (2 arrays: first array contains start token index and second array contains end token index)\n",
    "    \"\"\"\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"start_token_idx\": [],\n",
    "        \"end_token_idx\": [],\n",
    "    }\n",
    "    for item in tweet_examples:\n",
    "        if item.skip == False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "\n",
    "    x = [\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"token_type_ids\"],\n",
    "        dataset_dict[\"attention_mask\"],\n",
    "    ]\n",
    "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6bcedcb-ac35-44f3-8ed8-42e6271569e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, validation = train_test_split(data, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6daaaab1-cb73-4c27-948c-4f6d6e2e60ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>80d01af3a9</td>\n",
       "      <td>ha... I thought you would enjoy the Family Gu...</td>\n",
       "      <td>bored</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19020</th>\n",
       "      <td>7883967f6f</td>\n",
       "      <td>Bacon Fail.  Ended up with a commiseration muffin</td>\n",
       "      <td>Fail.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22843</th>\n",
       "      <td>cd8deca773</td>\n",
       "      <td>Walmart opening its first store in Amritsar to...</td>\n",
       "      <td>hoping it</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "3091   80d01af3a9   ha... I thought you would enjoy the Family Gu...   \n",
       "19020  7883967f6f  Bacon Fail.  Ended up with a commiseration muffin   \n",
       "22843  cd8deca773  Walmart opening its first store in Amritsar to...   \n",
       "\n",
       "      selected_text sentiment  \n",
       "3091          bored  negative  \n",
       "19020         Fail.  negative  \n",
       "22843     hoping it  positive  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "106a78c9-80f0-4349-8e72-b8a09e5e849c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't process 0 points\n",
      "14726 training points created.\n",
      "Couldn't process 0 points\n",
      "1637 evaluation points created.\n"
     ]
    }
   ],
   "source": [
    "train_tweet_examples = create_tweet_examples(train)\n",
    "x_train, y_train = create_inputs_targets(train_tweet_examples)\n",
    "print(f\"{len(train_tweet_examples)} training points created.\")\n",
    "\n",
    "eval_tweet_examples = create_tweet_examples(validation)\n",
    "x_eval, y_eval = create_inputs_targets(eval_tweet_examples)\n",
    "print(f\"{len(eval_tweet_examples)} evaluation points created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "942aa1d8-fd8e-4d9e-a7ca-7b21cc5d309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14726, 4) 14726\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, len(train_tweet_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "398a2338-1146-432a-9742-dd02a08dc222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative  ha... I thought you would enjoy the Family Guy reference... and I`m more bored than you [101, 5871, 119, 119, 119, 178, 1354, 1128, 1156, 5548, 1103, 1266, 2564, 3835, 119, 119, 119, 1105, 178, 169, 182, 1167, 11920, 1190, 1128, 102, 4366, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 22 22 [(0, 0), (0, 2), (2, 3), (3, 4), (4, 5), (6, 7), (8, 15), (16, 19), (20, 25), (26, 31), (32, 35), (36, 42), (43, 46), (47, 56), (56, 57), (57, 58), (58, 59), (60, 63), (64, 65), (65, 66), (66, 67), (68, 72), (73, 78), (79, 83), (84, 87), (0, 0)]\n",
      "negative Bacon Fail.  Ended up with a commiseration muffin [101, 20503, 8693, 119, 2207, 1146, 1114, 170, 3254, 26806, 6108, 182, 9435, 1394, 102, 4366, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 2 3 [(0, 0), (0, 5), (6, 10), (10, 11), (12, 17), (18, 20), (21, 25), (26, 27), (28, 31), (31, 35), (35, 41), (42, 43), (43, 46), (46, 48), (0, 0)]\n",
      "positive Walmart opening its first store in Amritsar tomorrow (of all places)...the horror..hoping its limited to wholesale [101, 20049, 16882, 3740, 2280, 1157, 1148, 2984, 1107, 1821, 26846, 1813, 4911, 113, 1104, 1155, 2844, 114, 119, 119, 119, 1103, 5367, 119, 119, 4717, 1157, 2609, 1106, 20767, 102, 3112, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 25 26 [(0, 0), (0, 2), (2, 5), (5, 7), (8, 15), (16, 19), (20, 25), (26, 31), (32, 34), (35, 37), (37, 41), (41, 43), (44, 52), (53, 54), (54, 56), (57, 60), (61, 67), (67, 68), (68, 69), (69, 70), (70, 71), (71, 74), (75, 81), (81, 82), (82, 83), (83, 89), (90, 93), (94, 101), (102, 104), (105, 114), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(train_tweet_examples[i].sentiment,\n",
    "          train_tweet_examples[i].text,\n",
    "          train_tweet_examples[i].input_ids,\n",
    "          train_tweet_examples[i].token_type_ids,\n",
    "          train_tweet_examples[i].attention_mask,\n",
    "          train_tweet_examples[i].start_token_idx,\n",
    "          train_tweet_examples[i].end_token_idx,\n",
    "          train_tweet_examples[i].context_token_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "21d244d7-411d-4740-a178-0e8fabe322b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (9753,)\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train), y_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "25a8c931-3ec2-410c-af8e-df0cec1e01e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  101,  5871,   119, ...,     0,     0,     0],\n",
      "       [  101, 20503,  8693, ...,     0,     0,     0],\n",
      "       [  101, 20049, 16882, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,   178,  1176, ...,     0,     0,     0],\n",
      "       [  101,  6476,   113, ...,     0,     0,     0],\n",
      "       [  101,  1139,  5919, ...,     0,     0,     0]])]\n",
      "[array([22,  2, 25, ..., 24, 24,  3]), array([22,  3, 26, ..., 24, 25,  6])]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:1])\n",
    "print(y_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d669619-6a0c-496c-a90c-0f8e8e0a130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't process 0 points\n",
      "3534 test points created.\n"
     ]
    }
   ],
   "source": [
    "test_tweet_examples = create_tweet_examples_test(test)\n",
    "x_test, _ = create_inputs_targets(test_tweet_examples)\n",
    "print(f\"{len(test_tweet_examples)} test points created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b91acd-f869-470b-b310-415b00ca2e84",
   "metadata": {},
   "source": [
    "### Creating & Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b651b17-bab5-4737-a3f7-d9cf973eb58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    PATH = 'kaggle/input/spanbert-pt/spanbert-base-cased/'\n",
    "    encoder = TFBertModel.from_pretrained(PATH, from_pt = True)\n",
    "    \n",
    "    ## QA Model\n",
    "    input_ids = layers.Input(shape=(max_len ,), dtype=tf.int32)\n",
    "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    embedding = encoder(\n",
    "         input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "    )[0]\n",
    "    \n",
    "\n",
    "    start_logits = layers.Dropout(0.3)(embedding)\n",
    "    start_logits = layers.Conv1D(128,2,padding='same')(start_logits)\n",
    "    start_logits = layers.LeakyReLU()(start_logits)\n",
    "    start_logits = layers.Conv1D(64,2,padding='same')(start_logits)\n",
    "    start_logits = layers.Dense(1, name=\"start_logit\")(start_logits)\n",
    "    start_logits = layers.Flatten()(start_logits)\n",
    "\n",
    "    end_logits = layers.Dropout(0.3)(embedding)\n",
    "    end_logits = layers.Conv1D(128,2,padding='same')(end_logits)\n",
    "    end_logits = layers.LeakyReLU()(end_logits)\n",
    "    end_logits = layers.Conv1D(64,2,padding='same')(end_logits)\n",
    "    end_logits = layers.Dense(1, name=\"end_logit\")(end_logits)\n",
    "    end_logits = layers.Flatten()(end_logits)\n",
    "\n",
    "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask],\n",
    "        outputs=[start_probs, end_probs],\n",
    "    )\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    optimizer = keras.optimizers.Adam(lr=5e-5)\n",
    "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3979ea01-444c-4149-9d87-4a32f8c12f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 17:40:41.611646: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-16 17:40:41.611687: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-16 17:40:41.611724: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-235-19-127.aws.lmig.com): /proc/driver/nvidia/version does not exist\n",
      "2022-08-16 17:40:41.612008: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All PyTorch model weights were used when initializing TFBertModel.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertModel were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_3[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]']                \n",
      "                                n_state=(None, 384,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 384, 768)     0           ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 384, 768)     0           ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 384, 128)     196736      ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 384, 128)     196736      ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 384, 128)     0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 384, 128)     0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 384, 64)      16448       ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 384, 64)      16448       ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " start_logit (Dense)            (None, 384, 1)       65          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " end_logit (Dense)              (None, 384, 1)       65          ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 384)          0           ['start_logit[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 384)          0           ['end_logit[0][0]']              \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 384)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 384)          0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,736,770\n",
      "Trainable params: 108,736,770\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/cortex-installs/miniconda/envs/cortex-python3/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "use_tpu = False\n",
    "if use_tpu:\n",
    "    # Create distribution strategy\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "    # Create model\n",
    "    with strategy.scope():\n",
    "        model = create_model()\n",
    "else:\n",
    "    model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64c2aed7-1143-4f0c-9012-aaf20d880787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuations\n",
    "    exclude = set(string.punctuation)\n",
    "    text = \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    # Remove articles\n",
    "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "    text = re.sub(regex, \" \", text)\n",
    "\n",
    "    # Remove extra white space\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "class ExactMatch(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Each `TweetExample` object contains the character level offsets for each token\n",
    "    in its input text. We use them to get back the span of text corresponding\n",
    "    to the tokens between our predicted start and end tokens.\n",
    "    All the ground-truth answers are also present in each `TweetExample` object.\n",
    "    We calculate the percentage of data points where the span of text obtained\n",
    "    from model predictions matches one of the ground-truth answers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
    "        count = 0\n",
    "        eval_examples_no_skip = [_ for _ in eval_tweet_examples if _.skip == False]\n",
    "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "            tweet_eg = eval_examples_no_skip[idx]\n",
    "            offsets = tweet_eg.context_token_to_char\n",
    "            start = np.argmax(start)\n",
    "            end = np.argmax(end)\n",
    "            if start >= len(offsets):\n",
    "                continue\n",
    "            pred_char_start = offsets[start][0]\n",
    "            if end < len(offsets):\n",
    "                pred_char_end = offsets[end][1]\n",
    "                pred_ans = tweet_eg.text[pred_char_start:pred_char_end]\n",
    "            else:\n",
    "                pred_ans = tweet_eg.text[pred_char_start:]\n",
    "\n",
    "            normalized_pred_ans = normalize_text(pred_ans)\n",
    "            normalized_true_ans = [normalize_text(_) for _ in tweet_eg.all_answers]\n",
    "            if normalized_pred_ans in normalized_true_ans:\n",
    "                count += 1\n",
    "        acc = count / len(self.y_eval[0])\n",
    "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df73c78e-bc44-4dbc-afbd-8cf3ec864932",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "div = len(x_train[0]) - (len(x_train[0]) % batch_size)\n",
    "x = list(np.array(x_train)[:,:div]) ## inputs must be divisible by batch size \n",
    "y = list(np.array(y_train)[:,:div])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "128c47de-9ce1-4f8b-b918-0ffa2c8e38da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/cortex-installs/miniconda/envs/cortex-python3/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "35/35 [==============================] - 500s 14s/step lo\n",
      "\n",
      "epoch=1, exact match score=0.55\n",
      "1218/1218 [==============================] - 13183s 11s/step - loss: 3.3840 - activation_loss: 1.5735 - activation_1_loss: 1.8105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa7328ae470>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
    "model.fit(\n",
    "    list(np.array(x)),\n",
    "    list(np.array(y)),\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[exact_match_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb3e56-93f5-4cda-a1f8-db8aea89e06d",
   "metadata": {},
   "source": [
    "### Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3319fe0-10cb-47c0-bc00-2447a821a319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 1600s 14s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3534, 384)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "np.array(pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab51fe51-fd65-4024-984e-e201e9754103",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['prediction'] = np.zeros(len(test.text.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5dbc82b-8c4c-417c-b4f0-09ef6cd8bed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3496/815857967.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['prediction'][idx] = normalized_pred_ans\n"
     ]
    }
   ],
   "source": [
    "pred_start, pred_end = pred\n",
    "count = 0\n",
    "\n",
    "pred_text = []\n",
    "for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "    tweet_eg = test_tweet_examples[idx]\n",
    "    if (tweet_eg.skip == True):\n",
    "        pred_text.append(\"\")\n",
    "    else: \n",
    "        offsets = tweet_eg.text_token_to_char\n",
    "        start = np.argmax(start)\n",
    "        end = np.argmax(end)\n",
    "        if start >= len(offsets):\n",
    "            continue\n",
    "        pred_char_start = offsets[start][0]\n",
    "        if end < len(offsets):\n",
    "            pred_char_end = offsets[end][1]\n",
    "            pred_ans = tweet_eg.text[pred_char_start:pred_char_end + 1]\n",
    "        else:\n",
    "            pred_ans = tweet_eg.text[pred_char_start:]\n",
    "\n",
    "        normalized_pred_ans = normalize_text(pred_ans)\n",
    "        #print(normalized_pred_ans)\n",
    "        pred_text.append(normalized_pred_ans)\n",
    "        test['prediction'][idx] = normalized_pred_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e10e376-7e77-4a33-8b24-3a6dc0332827",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.prediction[test.prediction == 0] = test.text[test.prediction == 0]\n",
    "test.prediction[test.sentiment == \"neutral\"] = test.text[test.sentiment==\"neutral\"]\n",
    "test.prediction[test.prediction ==''] = test.text[test.prediction =='']\n",
    "\n",
    "def clean_text(text):\n",
    "    words = str(text).split()\n",
    "    words = [x for x in words if not x.startswith(\"http\")]\n",
    "    words = \" \".join(words)\n",
    "    return words\n",
    "\n",
    "test['prediction'] = test['prediction'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0c0a35c-c582-46c5-99a5-31edbc4fad82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Last session of the day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>exciting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>shame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>726e501993</td>\n",
       "      <td>that`s great!! weee!! visitors!</td>\n",
       "      <td>positive</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>261932614e</td>\n",
       "      <td>I THINK EVERYONE HATES ME ON HERE   lol</td>\n",
       "      <td>negative</td>\n",
       "      <td>hates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>afa11da83f</td>\n",
       "      <td>soooooo wish i could, but im in school and my...</td>\n",
       "      <td>negative</td>\n",
       "      <td>wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e64208b4ef</td>\n",
       "      <td>and within a short time of the last clue all ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>and within a short time of the last clue all o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37bcad24ca</td>\n",
       "      <td>What did you get?  My day is alright.. haven`...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>What did you get? My day is alright.. haven`t ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment  \\\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3  01082688c6                                        happy bday!  positive   \n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
       "5  726e501993                    that`s great!! weee!! visitors!  positive   \n",
       "6  261932614e            I THINK EVERYONE HATES ME ON HERE   lol  negative   \n",
       "7  afa11da83f   soooooo wish i could, but im in school and my...  negative   \n",
       "8  e64208b4ef   and within a short time of the last clue all ...   neutral   \n",
       "9  37bcad24ca   What did you get?  My day is alright.. haven`...   neutral   \n",
       "\n",
       "                                          prediction  \n",
       "0                            Last session of the day  \n",
       "1                                           exciting  \n",
       "2                                              shame  \n",
       "3                                              happy  \n",
       "4                                               like  \n",
       "5                                              great  \n",
       "6                                              hates  \n",
       "7                                               wish  \n",
       "8  and within a short time of the last clue all o...  \n",
       "9  What did you get? My day is alright.. haven`t ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0574c27-f8f7-4e23-b131-50c347d2fa4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cortex-Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
