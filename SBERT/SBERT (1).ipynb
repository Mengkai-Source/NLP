{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55bbd0b0-9ed0-4478-92ee-5c702e863952",
   "metadata": {},
   "source": [
    "## SentenceBERT model training, contents include\n",
    "#### 1. Training data preparation\n",
    "    x: Input features are text (dataset_dict[\"input_ids\"], dataset_dict[\"token_type_ids\"], dataset_dict[\"attention_mask\"]) and sentiment label\n",
    "    y: Selected text/answer\n",
    "#### 2. Model training\n",
    "#### 3. Inference\n",
    "**Note: refer to original post for detail https://towardsdatascience.com/sbert-vs-data2vec-on-text-classification-e3c35b19c949**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec3848d-3259-4273-986e-68260078761e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/cortex-installs/miniconda/envs/cortex-python3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-08-17 14:01:13.420187: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-17 14:01:13.420235: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbaa01f-7af4-4001-b37b-08f687879743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bbc-text.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0726a7-0bfa-419b-95f3-d23a90a0878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if is_torch_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if is_tf_available():\n",
    "        import tensorflow as tf\n",
    " \n",
    "        tf.random.set_seed(seed)\n",
    " \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f10cf0c-9527-4215-96b4-f22e0108cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextClassification_with_Transformer(model_name: str, Data: pd.Series, Target:pd.Series, test_size: np.float64, max_length: int, num_labels: int, num_epochs: int, metrics_name: str):\n",
    "    \n",
    "    # Make data\n",
    "    X = Data\n",
    "    y = Target\n",
    "    y = pd.factorize(y)[0]\n",
    "\n",
    "    # Load Metrics\n",
    "    metric = load_metric(metrics_name)\n",
    "\n",
    "    # Split Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.tolist(), y, test_size=test_size)\n",
    "\n",
    "    # Call the Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "    # Encode the text\n",
    "    train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)\n",
    "    valid_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "\n",
    "\n",
    "    class MakeTorchData(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "            item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    # convert our tokenized data into a torch Dataset\n",
    "    train_dataset = MakeTorchData(train_encodings, y_train.ravel())\n",
    "    valid_dataset = MakeTorchData(valid_encodings, y_test.ravel())\n",
    "\n",
    "    # Call Model (Refere to \"https://stackoverflow.com/questions/67948945/force-bert-transformer-to-use-cuda\" to use GPU or CPU)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_labels) # .to(\"cuda\")\n",
    "\n",
    "    # Create Metrics\n",
    "    def compute_metrics(eval_pred):\n",
    "        \n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        # 'micro', 'macro', etc. are for multi-label classification. If you are running a binary classification, leave it as default or specify \"binary\" for average\n",
    "        return metric.compute(predictions=predictions, references=labels, average=\"micro\")  \n",
    "\n",
    "    # Specifiy the arguments for the trainer  \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=num_epochs,     # total number of training epochs\n",
    "        per_device_train_batch_size=8,   # batch size per device during training\n",
    "        per_device_eval_batch_size=20,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "        metric_for_best_model = metrics_name,    # select the base metrics\n",
    "        logging_steps=200,               # log & save weights each logging_steps\n",
    "        save_steps=200,\n",
    "        evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    "      ) \n",
    "    \n",
    "    # Call the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=valid_dataset,          # evaluation dataset\n",
    "        compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "      )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "  \n",
    "    # Call the summary\n",
    "    trainer.evaluate()\n",
    "\n",
    "\n",
    "\n",
    "    return trainer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691a8b5-4b9b-40d2-86ad-e31c6ca47639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/all-mpnet-base-v2 were not used when initializing MPNetForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/cortex-installs/miniconda/envs/cortex-python3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1490\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 935\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='122' max='935' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [122/935 47:57 < 5:24:54, 0.04 it/s, Epoch 0.65/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "sbert_trainer, sbert_model = TextClassification_with_Transformer(model_name = 'sentence-transformers/all-mpnet-base-v2', \n",
    "                                                                 Data = df.text, \n",
    "                                                                 Target = df.category, \n",
    "                                                                 test_size = 0.33, \n",
    "                                                                 max_length = 512, \n",
    "                                                                 num_labels = 5, \n",
    "                                                                 num_epochs = 5, \n",
    "                                                                 metrics_name='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a597614-88d2-4234-82ee-ff6a365cb039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cortex-Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
